{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7179e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e015fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss Function for CNN1\n",
    "loss_fn1 = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Mean Squared Error Function for CNN2\n",
    "loss_fn2 = torch.nn.MSELoss()\n",
    "\n",
    "# Adam Optimiser Function for Algorithm with Learning Rate of 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of random sequences for combination into 2D matrix for \n",
    "import random \n",
    "import torch\n",
    "list_numbers = random.choices([-1,1], k=160) \n",
    "\n",
    "S1 = torch.Tensor(list_numbers)\n",
    "\n",
    "\n",
    "list_numbers = random.choices([-1,1], k=160) \n",
    "\n",
    "S2 = torch.Tensor(list_numbers)\n",
    "\n",
    "\n",
    "S = S1 + S2\n",
    "print(S)\n",
    "input = S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4235c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fce6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc.tasks carried out per epoch\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a saved model\n",
    "saved_model = GarmentClassifier()\n",
    "saved_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "\n",
    "# Dataset and Dataloader\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation\n",
    "# Note: training and validations sets commented out as I was unable build the database required to provide datasets.\n",
    "# However, left them in as examples of implementation of training sets\n",
    "training_set = #torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = #torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Class labels - FH = Feasibly High Fidelity and Anti-FH = Non Feasibly High Fidelity\n",
    "classes = ('FH', 'Anti-FH')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9dcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#According to paper, CNN1 is a normal sequential network\n",
    "class CNN1(nn.Module):\n",
    "    #initialisation function to define the layers of the network\n",
    "    def __init__(self, in_channels, out_channels, downsample):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=7, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=7, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=7, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    #defines how the network will feed data forward through the layers   \n",
    "    def forward(self, input):\n",
    "        input = self.layer0(input)\n",
    "        input = self.layer1(input)\n",
    "        input = self.layer2(input)\n",
    "        input = self.layer3(input)\n",
    "        input = self.gap(input)\n",
    "        input = torch.flatten(input)\n",
    "        input = self.fc(input)\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d827b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#According to paper, CNN2 is a ResNet (Residual Network)\n",
    "\n",
    "#Defining ResidualBlock layers\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample):\n",
    "        super().__init__()\n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        shortcut = self.shortcut(input)\n",
    "        input = nn.ReLU()(self.bn1(self.conv1(input)))\n",
    "        input = nn.ReLU()(self.bn2(self.conv2(input)))\n",
    "        input = input + shortcut\n",
    "        return nn.ReLU()(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec0b18cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 0 consists of 7*7 convolution and a 3*3 max pooling\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, in_channels, resblock, outputs=1000):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            resblock(64, 64, downsample=False),\n",
    "            resblock(64, 64, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            resblock(64, 128, downsample=True),\n",
    "            resblock(128, 128, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            resblock(128, 256, downsample=True),\n",
    "            resblock(256, 256, downsample=False)\n",
    "        )\n",
    "#Layer 4 consists of a global average pooling (gap) and a fully connected layer (fc). GAP calculates the mean of each feature map and then concatenates all the values into a one-dimension list\n",
    "        self.layer4 = nn.Sequential(\n",
    "            resblock(256, 512, downsample=True),\n",
    "            resblock(512, 512, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = torch.nn.Linear(512, outputs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.layer0(input)\n",
    "        input = self.layer1(input)\n",
    "        input = self.layer2(input)\n",
    "        input = self.layer3(input)\n",
    "        input = self.layer4(input)\n",
    "        input = self.gap(input)\n",
    "        input = torch.flatten(input)\n",
    "        input = self.fc(input)\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b92689",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24216/3120254833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Prints full structure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresnet18\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mResBlock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "#Prints full structure\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "resnet18 = ResNet18(3, ResBlock, outputs=1000)\n",
    "resnet18.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "summary(resnet18, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c83db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e49dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
